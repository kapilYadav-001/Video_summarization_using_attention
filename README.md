# Video_summarization_using_attention


Key idea is to learn a deep summarization network with an attention mechanism to mimic the way of selecting the keyshots of a human. To this end, we propose a novel video summarization framework named Attentive encoder-decoder networks for Video Summarization (AVS), in which the encoder uses a Multi-layered Bidirectional Long Short-Term Memory (BiLSTM) to encode the contextual information among the input video frames. As for the decoder, a multi-layered attention-based LSTM network is explored by using a multiplicative objective function. The Research has been conducted on TvSum Dataset.
