# Video_summarization_using_attention


Key idea is to learn a deep summarization network with an attention mechanism to mimic the way of selecting the keyshots of a human. To this end, we propose a novel video summarization framework named Attentive encoder-decoder networks for Video Summarization (AVS), in which the encoder uses a Multi-layered Bidirectional Long Short-Term Memory (BiLSTM) to encode the contextual information among the input video frames. As for the decoder, a multi-layered attention-based LSTM network is explored by using a multiplicative objective function. The Research has been conducted on TvSum Dataset.<br>
<h1>outline of model</h1>

![image](https://github.com/kapilYadav-001/Video_summarization_using_attention/assets/58760022/666352c5-881c-4a99-9c85-ad617e6cceeb)
